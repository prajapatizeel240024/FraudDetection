{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef65c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the librarys\n",
    "import pandas as pd #To work with dataset\n",
    "import numpy as np #Math library\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns #Graph library that use matplot in background\n",
    "import matplotlib.pyplot as plt #to plot some parameters in seaborn\n",
    "import warnings\n",
    "# Preparation  \n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# Import StandardScaler from scikit-learn\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer,IterativeImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer,ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\n",
    "from sklearn.manifold import TSNE\n",
    "# Import train_test_split()\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "from sklearn.metrics import mean_squared_error,classification_report\n",
    "from sklearn.metrics import roc_curve,confusion_matrix\n",
    "from datetime import datetime, date\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#import tensorflow as tf \n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#import smogn\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# For training random forest model\n",
    "import lightgbm as lgb\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans \n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold,TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Feature Selection \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression,f_classif,chi2\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier,LGBMRegressor\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import set_config\n",
    "from itertools import combinations\n",
    "# Cluster :\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "#from yellowbrick.cluster import KElbowVisualizer\n",
    "#import smong \n",
    "import category_encoders as ce\n",
    "import warnings\n",
    "#import optuna \n",
    "from joblib import Parallel, delayed\n",
    "import joblib \n",
    "from sklearn import set_config\n",
    "from typing import List, Optional, Union\n",
    "set_config(display='diagram')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d16a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:\\\\Users\\\\praja\\\\PycharmProjects\\\\FraudDetection\\\\Data\\\\transactions_train.csv\")\n",
    "# Preview the data\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le train test\n",
    "target= \"isFraud\"\n",
    "X = train.drop(target, axis='columns')# axis=1\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isFraud.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select non-numeric columns\n",
    "cat_columns = X.select_dtypes(exclude=['int64','int16','float32','float64','int8']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the float columns\n",
    "num_columns = X.select_dtypes(include=['int64','int16','float32','float64','int8']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = (num_columns.append(cat_columns))\n",
    "print(cat_columns)\n",
    "print(num_columns)\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ab21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set(all_columns) == set(X.columns):\n",
    "    print('Ok')\n",
    "else:\n",
    "    # Let's see the difference \n",
    "    print('in all_columns but not in  train  :', set(all_columns) - set(X.columns))\n",
    "    print('in X.columns   but not all_columns :', set(X.columns) - set(all_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9293c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #return np.array(X)[:, self.positions]\n",
    "        return X.loc[:, self.positions] \n",
    "########################################################################\n",
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    # https://towardsdatascience.com/how-to-write-powerful-code-others-admire-with-custom-sklearn-transformers-34bc9087fdd\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = np.copy(X) + 1\n",
    "        self._estimator.fit(X_copy)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = np.copy(X) + 1\n",
    "\n",
    "        return self._estimator.transform(X_copy)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X_reversed = self._estimator.inverse_transform(np.copy(X))\n",
    "\n",
    "        return X_reversed - 1  \n",
    "\n",
    "class TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Temporal elapsed time transformer\n",
    "\n",
    "    def __init__(self, variables, reference_variable):\n",
    "        \n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "        \n",
    "        self.variables = variables\n",
    "        self.reference_variable = reference_variable\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need this step to fit the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "       # so that we do not over-write the original dataframe\n",
    "        X = X.copy()\n",
    "        \n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[self.reference_variable] - X[feature]\n",
    "\n",
    "        return X\n",
    "class CustomImputer(BaseEstimator, TransformerMixin) : \n",
    "    def __init__(self, variable, by) : \n",
    "            #self.something enables you to include the passed parameters\n",
    "            #as object attributes and use it in other methods of the class\n",
    "            self.variable = variable\n",
    "            self.by = by\n",
    "\n",
    "    def fit(self, X, y=None) : \n",
    "        self.map = X.groupby(self.by)[variable].mean()\n",
    "        #self.map become an attribute that is, the map of values to\n",
    "        #impute in function of index (corresponding table, like a dict)\n",
    "        return self\n",
    "\n",
    "def transform(self, X, y=None) : \n",
    "    X[variable] = X[variable].fillna(value = X[by].map(self.map))\n",
    "    #Change the variable column. If the value is missing, value should \n",
    "    #be replaced by the mapping of column \"by\" according to the map you\n",
    "    #created in fit method (self.map)\n",
    "    return X\n",
    "\n",
    "    # categorical missing value imputer\n",
    "class Mapper(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, variables, mappings):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "\n",
    "        self.variables = variables\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[feature].map(self.mappings)\n",
    "\n",
    "        return X  \n",
    "    \n",
    "##########################################################################\n",
    "class CountFrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    #temp = df['card1'].value_counts().to_dict()\n",
    "    #df['card1_counts'] = df['card1'].map(temp)\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding_method: str = \"count\",\n",
    "        variables: Union[None, int, str, List[Union[str, int]]] = None,\n",
    "        keep_variable=True,\n",
    "                  ) -> None:\n",
    "\n",
    "        self.encoding_method = encoding_method\n",
    "        self.variables = variables\n",
    "        self.keep_variable=keep_variable\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        Learn the counts or frequencies which will be used to replace the categories.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "            The training dataset. Can be the entire dataframe, not just the\n",
    "            variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "            y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        self.encoder_dict_ = {}\n",
    "\n",
    "        # learn encoding maps\n",
    "        for var in self.variables:\n",
    "            if self.encoding_method == \"count\":\n",
    "                self.encoder_dict_[var] = X[var].value_counts().to_dict()\n",
    "\n",
    "            elif self.encoding_method == \"frequency\":\n",
    "                n_obs = float(len(X))\n",
    "                self.encoder_dict_[var] = (X[var].value_counts() / n_obs).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # replace categories by the learned parameters\n",
    "        X = X.copy()\n",
    "        for feature in self.encoder_dict_.keys():\n",
    "            if self.keep_variable:\n",
    "                X[feature+'_fq_enc'] = X[feature].map(self.encoder_dict_[feature])\n",
    "            else:\n",
    "                X[feature] = X[feature].map(self.encoder_dict_[feature])\n",
    "        return X[self.variables].to_numpy()\n",
    "#################################################   \n",
    "class FeaturesEngineerGroup(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,groupping_method =\"mean\",\n",
    "                   variables=  \"amount\",\n",
    "                   groupby_variables = \"nameOrig\"                         \n",
    "                 ) :\n",
    "        self.groupping_method = groupping_method\n",
    "        self.variables=variables\n",
    "        self.groupby_variables=groupby_variables\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "        The training dataset. Can be the entire dataframe, not just the\n",
    "        variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "        y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        self.group_amount_dict_ = {}\n",
    "        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n",
    "        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "        #df = pd.merge(df,temp,on='card1',how='left')\n",
    "        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n",
    "        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n",
    "        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n",
    "        # learn mean/medain \n",
    "        #for groupby in self.groupby_variables:\n",
    "         #   for var in self.variables:\n",
    "        if self.groupping_method == \"mean\":\n",
    "            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['mean']).to_dict()\n",
    "        elif self.groupping_method == \"median\":\n",
    "            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['median']).to_dict()\n",
    "        else:\n",
    "            print('error , chose mean or median')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        #for col in self.variables:\n",
    "         #   for agg_type in self.groupping_method:\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n",
    "        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_[ self.variables][self.groupping_method])\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)    \n",
    "    \n",
    "################################################   \n",
    "class FeaturesEngineerGroup2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,groupping_method =\"mean\",\n",
    "                   variables=  \"amount\",\n",
    "                   groupby_variables = \"nameOrig\"                         \n",
    "                 ) :\n",
    "        self.groupping_method = groupping_method\n",
    "        self.variables=variables\n",
    "        self.groupby_variables=groupby_variables\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "        The training dataset. Can be the entire dataframe, not just the\n",
    "        variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "        y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        self.group_amount_dict_ = {}\n",
    "        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n",
    "        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "        #df = pd.merge(df,temp,on='card1',how='left')\n",
    "        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n",
    "        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n",
    "        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n",
    "        # learn mean/medain \n",
    "        #for groupby in self.groupby_variables:\n",
    "         #   for var in self.variables:\n",
    "\n",
    "        print('we have {} unique clients'.format(X[self.groupby_variables].nunique()))\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method    \n",
    "        X[new_col_name] = X.groupby([self.groupby_variables])[[self.variables]].transform(self.groupping_method)\n",
    "        X = X.drop_duplicates(['nameOrig'])\n",
    "    \n",
    "        self.group_amount_dict_ = dict(zip(X[self.groupby_variables], X[new_col_name]))\n",
    "        del X\n",
    "        #print('we have {} unique mean amount : one for each client'.format(len(self.group_amount_dict_)))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        #for col in self.variables:\n",
    "         #   for agg_type in self.groupping_method:\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n",
    "        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_)\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)   \n",
    "    \n",
    "############################################  \n",
    "class FeaturesEngineerCumCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,group_one =\"step\",\n",
    "                   group_two=  \"nameOrig\"                       \n",
    "                 ) :\n",
    "        self.group_one =group_one\n",
    "        self.group_two=group_two\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        new_col_name =  self.group_two+'_Transaction_count'\n",
    "        X[new_col_name] = X.groupby([self.group_one, self.group_two])[[self.group_two]].transform('count')\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27087bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat columns: \n",
    "cat_pipe = Pipeline([\n",
    "                     ('Encoder',ce.target_encoder.TargetEncoder())\n",
    "                     \n",
    "                    ])\n",
    "#Num_columns:\n",
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median',add_indicator=False)),\n",
    "                     ('scaler', QuantileTransformer())\n",
    "                    ])\n",
    "#Feature Union fitting training data :\n",
    "preprocessor = FeatureUnion(transformer_list=[('cat', cat_pipe),\n",
    "                                              ('num', num_pipe)])\n",
    "# Using ColumnTransformer:\n",
    "data_cleaning = ColumnTransformer([\n",
    "    ('cat_columns',  cat_pipe, cat_columns ),\n",
    "    ('num_columns', num_pipe , num_columns)\n",
    "])\n",
    "# preprocessor.fit(X_train)\n",
    "#############################\n",
    "# Complete Pipe \n",
    "def create_pipeline(model,preprocessor,FeaturesEngineer=None):\n",
    "    pipeline = Pipeline([ \n",
    "        ('pre', preprocessor),\n",
    "        ('lgbm', model)\n",
    "    ])\n",
    "    return pipeline\n",
    "preprocessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1468016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete pipe :\n",
    "# select the float/cat columns\n",
    "#cat_feautres = X.select_dtypes(include=['object','category']).columns\n",
    "#num_features = X.select_dtypes(exclude=['object','category']).columns\n",
    "#Define vcat pipeline\n",
    "features_cum_count=['step','nameOrig']\n",
    "features_groupby_amount=['amount','nameOrig']\n",
    "features_frequency_orig_dest=['nameOrig','nameDest']\n",
    "features_cum_count_pipe = Pipeline([\n",
    "                     ('transformer_Encoder', FeaturesEngineerCumCount())\n",
    "                    ])\n",
    "features_groupby_pipe = Pipeline([\n",
    "                     ('transformer_group_amount_mean', FeaturesEngineerGroup2()),\n",
    "                     ('transformer_group_scaler', PowerTransformer())\n",
    "                    ])\n",
    "features_frequency_pipe = Pipeline([\n",
    "                     ('Encoder', CountFrequencyEncoder(variables=['nameOrig','nameDest'],encoding_method =\"frequency\", keep_variable=False))\n",
    "                    ])\n",
    "type_pipe= Pipeline([\n",
    "                     ('transformer_Encoder', ce.cat_boost.CatBoostEncoder())\n",
    "                    ])\n",
    "num_features0=[  'amount',  'oldbalanceOrig', 'newbalanceOrig' ,'oldbalanceDest', 'newbalanceDest']\n",
    "#Define vnum pipeline\n",
    "num_pipe = Pipeline([\n",
    "                     ('scaler', PowerTransformer()),\n",
    "                    ])\n",
    "#Featureunion fitting training data\n",
    "preprocessor = FeatureUnion(transformer_list=[('cum_count', features_cum_count_pipe),\n",
    "                                              ('mean_amount', features_groupby_pipe),\n",
    "                                              ('frequency_dest_orig', features_frequency_pipe),\n",
    "                                              ('trans_type', type_pipe),\n",
    "                                              ('num', num_pipe)])\n",
    "data_preparing= ColumnTransformer([\n",
    "    ('cum_count', features_cum_count_pipe, features_cum_count ),\n",
    "    ('mean_amount', features_groupby_pipe, features_groupby_amount ),\n",
    "    ('frequency_dest_orig', features_frequency_pipe, features_frequency_orig_dest ),\n",
    "    ('trans_type', type_pipe, ['type'] ),\n",
    "    ('num', num_pipe, num_features0)\n",
    "], remainder='drop')\n",
    "data_preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_xgb ={'eval_metric':'auc',\n",
    "                'max_depth': 6, \n",
    "                'colsample_bytree': 0.7098433872257219, \n",
    "                'min_child_weight': 482, \n",
    "                'subsample': 0.8406820875269025, \n",
    "                'reg_alpha': 3.2594867475105374, \n",
    "                'reg_lambda': 0.1534227221930378,\n",
    "                'random_state': 666,\n",
    "                'n_jobs' : -1,\n",
    "                'n_estimators' : 4000,\n",
    "                'learning_rate' : 0.005,\n",
    "                'tree_method': \"gpu_hist\",\n",
    "                'predictor' :\"gpu_predictor\",\n",
    "                'gpu_id' : 0}\n",
    "            \n",
    "model_xgb0 = XGBClassifier( \n",
    "        n_estimators=2000,\n",
    "        max_depth=12, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        missing=-1, \n",
    "        eval_metric='auc',\n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist' \n",
    "        # USE GPU\n",
    "        tree_method='gpu_hist',\n",
    "        predictor =\"gpu_predictor\",\n",
    "        gpu_id = 0\n",
    "    )\n",
    "model_xgb = XGBClassifier(\n",
    "      #objective=\"mae\",\n",
    "    **param_xgb)\n",
    "param_lgbm ={'metric': 'auc',\n",
    "             \"device_type\" : \"gpu\",\n",
    "              'boosting_type': 'gbdt',\n",
    "              'lambda_l1': 1.363777251241345e-08,\n",
    "              'lambda_l2': 4.62760154881154e-05,\n",
    "              'num_leaves': 220,\n",
    "              'n_estimators': 11544,\n",
    "              'reg_alpha': 0.2364285339007138,\n",
    "              'reg_lambda': 0.2640697497357361,\n",
    "              'colsample_bytree': 1.0,\n",
    "              'subsample': 0.7,\n",
    "              'learning_rate': 0.014,\n",
    "              'tree_method': \"gpu_hist\",\n",
    "              'max_depth': 100, \n",
    "              'min_child_samples': 128,\n",
    "              'min_data_per_groups': 36} \n",
    "model_lgbm = LGBMClassifier(\n",
    "                        **param_lgbm,\n",
    "                       random_state = 0,\n",
    "                      )\n",
    "    \n",
    "pipeline_model_xgb = Pipeline([\n",
    "        ('pre', data_preparing),\n",
    "        ('estimator', model_xgb0)\n",
    "    ])\n",
    "pipeline_model_lgbm = Pipeline([\n",
    "        ('pre', data_preparing),\n",
    "        ('estimator', model_lgbm)\n",
    "    ])\n",
    "pipeline_model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "# Setting a 10-fold stratified cross-validation (note: shuffle=True)\n",
    "SEED = 42\n",
    "FOLDS = 2\n",
    "# CV interations\n",
    "# Create arrays for the features and the response variable\n",
    "roc_auc = list()\n",
    "F1 = list()\n",
    "average_precision = list()\n",
    "oof     = np.empty((X.shape[0],))\n",
    "oof_bin = np.empty((X.shape[0],))\n",
    "predictions=[]\n",
    "mean_auc = 0\n",
    "best_iteration = list()\n",
    "for fold, (train_idx, test_idx) in enumerate(GroupTimeSeriesSplit(n_splits=FOLDS).split(X,y ,groups=X['step'])):\n",
    "    print('-' * 80)\n",
    "    print('Fold: ', fold)\n",
    "    print('Groups for training:')\n",
    "    print(X.loc[train_idx, 'step'].unique())\n",
    "    print(X.loc[train_idx, :].shape)\n",
    "    print(y.loc[train_idx].shape)\n",
    "    print('Group for test:')\n",
    "    print(X.loc[test_idx, 'step'].unique())\n",
    "    print('-' * 80)  \n",
    "    X_train, y_train = X.iloc[list(train_idx), :], y.iloc[list(train_idx)]\n",
    "    X_test, y_test = X.iloc[list(test_idx), :], y.iloc[list(test_idx)]\n",
    "    pipeline_model_lgbm.fit( X_train, y_train,\n",
    "                             #estimator__eval_metric=\"auc\",\n",
    "                             #estimator__eval_set = [(X_test, y_test)],\n",
    "                             #estimator__verbose=1000,\n",
    "                             #estimator__early_stopping_rounds =150\n",
    "                            # lgbm__sample_weight=X_train_weight\n",
    "                           )\n",
    "    #pipeline_model_lgbm.fit(X_train,y_train)\n",
    "    preds = pipeline_model_lgbm.predict_proba(X_test)[:,1]\n",
    "    #oof[test_idx] = preds\n",
    "    auc_score = roc_auc_score(y_true=y_test, y_score= preds)\n",
    "    average_precesion = average_precision_score(y_true= y_test, y_score= preds)\n",
    "    y_predicted = pipeline_model_lgbm.predict(X_test)\n",
    "    #recall = recall_score(y_test, y_predicted)\n",
    "    f1= f1_score(y_test, y_predicted)\n",
    "    print('Classification report:\\n',classification_report(y_test,y_predicted))\n",
    "    print('Confusion_matrix:\\n',confusion_matrix(y_test,y_predicted))\n",
    "    print(f\"Fold {fold} | AUC: {auc_score}\")\n",
    "    print(f\"Fold {fold} | Avergae_precesion: {average_precesion}\")\n",
    "    #print(f\"Fold {fold} | recall: {recall}\")\n",
    "    print(f\"Fold {fold} | F1: {f1}\")\n",
    "    roc_auc.append(auc_score)\n",
    "    F1.append(f1)\n",
    "    average_precision.append(average_precesion)\n",
    "    mean_auc += auc_score / FOLDS\n",
    "    #predictions.append(pipeline_model_HGBC.predict_proba(x_test_final)[:,1]) \n",
    "#Mean of the predictions\n",
    "print('-' * 80)  \n",
    "print(f\"\\nOverall mean F1 score : {np.mean(f1)}\")\n",
    "print(f\"\\nOverall mean AUC score : {mean_auc}\")\n",
    "print(f\"\\nOverall mean average precision score : {np.mean(average_precision)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6d2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a20e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3739da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bb6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766add6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d844d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220f832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
